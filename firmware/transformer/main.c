/* Tiny transformer character-level LM demo (float32).
 *
 * Reads a prompt from test_tokens, then autoregressively generates
 * tokens until the context window (32 positions) is full.  Each
 * generated token is printed as its raw character.
 *
 * Architecture: embed_dim=64, heads=4, layers=2, vocab=256, ctx=32
 * All weights and activations are float32. Uses FP NPU instructions
 * (opcode 0x2B) via npu_fp.h intrinsics.
 */

#include "../common/npu_fp.h"

/* Weights header (generated by riscv_npu.tools.export_transformer_weights) */
#include "weights.h"

/* Syscall wrappers */
long write(int fd, const void *buf, long len);
void _exit(int code);

/* Input buffer: test harness writes token IDs here.
 * test_n_tokens: prompt length.
 * test_n_generate: tokens to generate (0 = fill context). */
unsigned char test_tokens[CONTEXT_LEN];
int test_n_tokens;
int test_n_generate;

/* Scratch buffers (in .bss) */
static float x[EMBED_DIM];           /* Current token embedding */
static float normed[EMBED_DIM];      /* After RMSNorm */
static float q_proj[EMBED_DIM];      /* Q projection */
static float k_proj[EMBED_DIM];      /* K projection */
static float v_proj[EMBED_DIM];      /* V projection */
static float attn_out[EMBED_DIM];    /* Attention output */
static float ff_hidden[FF_DIM];      /* FFN hidden */

/* KV cache: (n_layers, context_len, embed_dim) */
static float k_cache[N_LAYERS][CONTEXT_LEN][EMBED_DIM];
static float v_cache[N_LAYERS][CONTEXT_LEN][EMBED_DIM];

/* Temporary float buffers for softmax */
static float scores_buf[CONTEXT_LEN];
static float probs_buf[CONTEXT_LEN];

/* ------------------------------------------------------------------ */
/* Utility                                                             */
/* ------------------------------------------------------------------ */

static void print_char(unsigned char c) {
    write(1, &c, 1);
}

/* ------------------------------------------------------------------ */
/* Linear layer: out[i] = dot(weight[i], in) + bias[i]                */
/* ------------------------------------------------------------------ */

static void linear(
    const float *input, int in_dim,
    const float *weight, const float *bias,
    float *output, int out_dim
) {
    for (int i = 0; i < out_dim; i++) {
        NPU_FVMAC(&weight[i * in_dim], input, in_dim);
        output[i] = NPU_FRSTACC() + bias[i];
    }
}

/* ------------------------------------------------------------------ */
/* RMSNorm: out = x * gamma * rsqrt(mean(x^2) + eps)                  */
/* ------------------------------------------------------------------ */

static void rmsnorm(
    const float *input, const float *gamma,
    float *output, int dim
) {
    /* sum of squares via FVMAC(input, input) */
    NPU_FVMAC(input, input, dim);
    float sum_sq = NPU_FRSTACC();
    float mean_sq = sum_sq / (float)dim + 1e-5f;

    /* 1/sqrt(mean_sq) */
    float scale = NPU_FVRSQRT(&mean_sq);

    /* output[i] = input[i] * gamma[i] * scale */
    for (int i = 0; i < dim; i++) {
        output[i] = input[i] * gamma[i] * scale;
    }
}

/* ------------------------------------------------------------------ */
/* Softmax: scores[] -> float probabilities                            */
/* ------------------------------------------------------------------ */

static void softmax(float *scores, int n, float *probs) {
    float max_score = NPU_FVMAX(scores, n);

    /* subtract max */
    for (int i = 0; i < n; i++)
        scores[i] -= max_score;

    /* exp */
    NPU_FVEXP(scores, probs, n);

    /* sum */
    float sum_exp = NPU_FVREDUCE(probs, n);

    /* normalize: set facc = 1/sum, then FVMUL */
    NPU_FRSTACC();                       /* clear facc */
    float inv = 1.0f / sum_exp;
    float one = 1.0f;
    NPU_FMACC(inv, one);                /* facc = 1/sum */
    NPU_FVMUL(probs, probs, n);         /* probs[i] *= facc */
    NPU_FRSTACC();                      /* clear facc so callers start clean */
}

/* ------------------------------------------------------------------ */
/* GELU activation                                                     */
/* ------------------------------------------------------------------ */

static void gelu_activation(float *buf, int n) {
    for (int i = 0; i < n; i++)
        buf[i] = NPU_FGELU(buf[i]);
}

/* ------------------------------------------------------------------ */
/* Multi-head attention                                                */
/* ------------------------------------------------------------------ */

static void attention(
    const float *x_in,
    int layer,
    int pos,
    const float *wq, const float *bq,
    const float *wk, const float *bk,
    const float *wv, const float *bv,
    const float *wo, const float *bo,
    float *output
) {
    /* Project to Q, K, V */
    linear(x_in, EMBED_DIM, wq, bq, q_proj, EMBED_DIM);
    linear(x_in, EMBED_DIM, wk, bk, k_proj, EMBED_DIM);
    linear(x_in, EMBED_DIM, wv, bv, v_proj, EMBED_DIM);

    /* Store K, V into cache at position pos */
    for (int d = 0; d < EMBED_DIM; d++) {
        k_cache[layer][pos][d] = k_proj[d];
        v_cache[layer][pos][d] = v_proj[d];
    }

    int n_tokens = pos + 1;
    float head_dim_f = (float)HEAD_DIM;
    float attn_scale = NPU_FVRSQRT(&head_dim_f);

    /* Per-head attention */
    for (int h = 0; h < N_HEADS; h++) {
        int h_start = h * HEAD_DIM;

        /* Compute attention scores: Q[h] . K_cache[h] * scale */
        for (int t = 0; t < n_tokens; t++) {
            NPU_FVMAC(&q_proj[h_start], &k_cache[layer][t][h_start], HEAD_DIM);
            scores_buf[t] = NPU_FRSTACC() * attn_scale;
        }

        /* Softmax */
        softmax(scores_buf, n_tokens, probs_buf);

        /* Weighted sum of V */
        for (int d = 0; d < HEAD_DIM; d++) {
            float acc = 0.0f;
            for (int t = 0; t < n_tokens; t++) {
                acc += probs_buf[t] * v_cache[layer][t][h_start + d];
            }
            attn_out[h_start + d] = acc;
        }
    }

    /* Output projection */
    linear(attn_out, EMBED_DIM, wo, bo, output, EMBED_DIM);
}

/* ------------------------------------------------------------------ */
/* Feedforward: GELU(x @ W1 + b1) @ W2 + b2                           */
/* ------------------------------------------------------------------ */

static void feedforward(
    const float *x_in,
    const float *w1, const float *b1,
    const float *w2, const float *b2,
    float *output
) {
    /* First linear: dim -> ff_dim */
    linear(x_in, EMBED_DIM, w1, b1, ff_hidden, FF_DIM);

    /* GELU activation */
    gelu_activation(ff_hidden, FF_DIM);

    /* Second linear: ff_dim -> dim */
    linear(ff_hidden, FF_DIM, w2, b2, output, EMBED_DIM);
}

/* ------------------------------------------------------------------ */
/* Add residual                                                        */
/* ------------------------------------------------------------------ */

static void add_residual(float *x_inout, const float *residual, int dim) {
    for (int i = 0; i < dim; i++) {
        x_inout[i] = x_inout[i] + residual[i];
    }
}

/* ------------------------------------------------------------------ */
/* Forward: run one token through the transformer at position pos      */
/* ------------------------------------------------------------------ */

static void forward_step(unsigned char token, int pos) {
    /* Embedding: token_embed[token] + pos_embed[pos] */
    for (int d = 0; d < EMBED_DIM; d++) {
        x[d] = TOKEN_EMBED[token][d] + POS_EMBED[pos][d];
    }

    /* Layer 0 */
    {
        float residual[EMBED_DIM];
        for (int d = 0; d < EMBED_DIM; d++) residual[d] = x[d];

        rmsnorm(x, L0_LN1_GAMMA, normed, EMBED_DIM);
        float attn_result[EMBED_DIM];
        attention(normed, 0, pos,
                  (const float *)L0_WQ, L0_BQ,
                  (const float *)L0_WK, L0_BK,
                  (const float *)L0_WV, L0_BV,
                  (const float *)L0_WO, L0_BO,
                  attn_result);
        for (int d = 0; d < EMBED_DIM; d++) x[d] = attn_result[d];
        add_residual(x, residual, EMBED_DIM);

        for (int d = 0; d < EMBED_DIM; d++) residual[d] = x[d];
        rmsnorm(x, L0_LN2_GAMMA, normed, EMBED_DIM);
        float ff_result[EMBED_DIM];
        feedforward(normed,
                    (const float *)L0_W1, L0_B1,
                    (const float *)L0_W2, L0_B2,
                    ff_result);
        for (int d = 0; d < EMBED_DIM; d++) x[d] = ff_result[d];
        add_residual(x, residual, EMBED_DIM);
    }

    /* Layer 1 */
    {
        float residual[EMBED_DIM];
        for (int d = 0; d < EMBED_DIM; d++) residual[d] = x[d];

        rmsnorm(x, L1_LN1_GAMMA, normed, EMBED_DIM);
        float attn_result[EMBED_DIM];
        attention(normed, 1, pos,
                  (const float *)L1_WQ, L1_BQ,
                  (const float *)L1_WK, L1_BK,
                  (const float *)L1_WV, L1_BV,
                  (const float *)L1_WO, L1_BO,
                  attn_result);
        for (int d = 0; d < EMBED_DIM; d++) x[d] = attn_result[d];
        add_residual(x, residual, EMBED_DIM);

        for (int d = 0; d < EMBED_DIM; d++) residual[d] = x[d];
        rmsnorm(x, L1_LN2_GAMMA, normed, EMBED_DIM);
        float ff_result[EMBED_DIM];
        feedforward(normed,
                    (const float *)L1_W1, L1_B1,
                    (const float *)L1_W2, L1_B2,
                    ff_result);
        for (int d = 0; d < EMBED_DIM; d++) x[d] = ff_result[d];
        add_residual(x, residual, EMBED_DIM);
    }
}

/* ------------------------------------------------------------------ */
/* Predict: run final norm + output projection, return argmax token    */
/* ------------------------------------------------------------------ */

static int predict(void) {
    rmsnorm(x, LN_FINAL_GAMMA, normed, EMBED_DIM);

    float max_logit = -1e30f;
    int best = 0;
    for (int v = 0; v < VOCAB_SIZE; v++) {
        NPU_FVMAC(&OUTPUT_PROJ[v][0], normed, EMBED_DIM);
        float logit = NPU_FRSTACC() + OUTPUT_BIAS[v];
        if (logit > max_logit) {
            max_logit = logit;
            best = v;
        }
    }
    return best;
}

/* ------------------------------------------------------------------ */
/* Main: process prompt, then generate tokens autoregressively         */
/* ------------------------------------------------------------------ */

int main(void) {
    int prompt_len = test_n_tokens;
    if (prompt_len <= 0 || prompt_len > CONTEXT_LEN) {
        _exit(1);
    }

    /* How many tokens to generate (default: fill the context window) */
    int n_gen = test_n_generate;
    if (n_gen <= 0) {
        n_gen = CONTEXT_LEN;
    }

    /* Echo the prompt */
    write(1, test_tokens, prompt_len);
    write(1, "\n> ", 3);

    /* Process prompt tokens */
    for (int pos = 0; pos < prompt_len; pos++) {
        forward_step(test_tokens[pos], pos);
    }

    /* Autoregressive generation */
    for (int i = 0; i < n_gen; i++) {
        int token = predict();
        print_char((unsigned char)token);

        /* Feed predicted token back; stop if context window is full */
        int next_pos = prompt_len + i;
        if (next_pos >= CONTEXT_LEN) {
            break;
        }
        forward_step((unsigned char)token, next_pos);
    }
    print_char('\n');

    return 0;
}
