/* Tiny transformer character-level LM demo (float32).
 *
 * Reads input tokens from the test_tokens buffer (written by the test
 * harness), runs one forward pass of the float32 transformer, and
 * prints the predicted next token (byte value 0-255) to stdout.
 *
 * Architecture: embed_dim=64, heads=4, layers=2, vocab=256, ctx=32
 * All weights and activations are float32. Uses FP NPU instructions
 * (opcode 0x2B) via npu_fp.h intrinsics.
 */

#include "../common/npu_fp.h"

/* Weights header (generated by riscv_npu.tools.export_transformer_weights) */
#include "weights.h"

/* Syscall wrappers */
long write(int fd, const void *buf, long len);
void _exit(int code);

/* Input buffer: test harness writes token IDs here.
 * test_n_tokens: number of tokens to process. */
unsigned char test_tokens[CONTEXT_LEN];
int test_n_tokens;

/* Scratch buffers (in .bss) */
static float x[EMBED_DIM];           /* Current token embedding */
static float normed[EMBED_DIM];      /* After RMSNorm */
static float q_proj[EMBED_DIM];      /* Q projection */
static float k_proj[EMBED_DIM];      /* K projection */
static float v_proj[EMBED_DIM];      /* V projection */
static float attn_out[EMBED_DIM];    /* Attention output */
static float ff_hidden[FF_DIM];      /* FFN hidden */

/* KV cache: (n_layers, context_len, embed_dim) */
static float k_cache[N_LAYERS][CONTEXT_LEN][EMBED_DIM];
static float v_cache[N_LAYERS][CONTEXT_LEN][EMBED_DIM];

/* Temporary float buffers for softmax */
static float scores_buf[CONTEXT_LEN];
static float probs_buf[CONTEXT_LEN];

/* ------------------------------------------------------------------ */
/* Utility                                                             */
/* ------------------------------------------------------------------ */

static void print_int(int n) {
    char buf[8];
    int len = 0;
    if (n >= 100) {
        buf[len++] = '0' + (n / 100);
        n %= 100;
        buf[len++] = '0' + (n / 10);
        buf[len++] = '0' + (n % 10);
    } else if (n >= 10) {
        buf[len++] = '0' + (n / 10);
        buf[len++] = '0' + (n % 10);
    } else {
        buf[len++] = '0' + n;
    }
    buf[len++] = '\n';
    write(1, buf, len);
}

/* ------------------------------------------------------------------ */
/* Linear layer: out[i] = dot(weight[i], in) + bias[i]                */
/* ------------------------------------------------------------------ */

static void linear(
    const float *input, int in_dim,
    const float *weight, const float *bias,
    float *output, int out_dim
) {
    for (int i = 0; i < out_dim; i++) {
        NPU_FVMAC(&weight[i * in_dim], input, in_dim);
        output[i] = NPU_FRSTACC() + bias[i];
    }
}

/* ------------------------------------------------------------------ */
/* RMSNorm: out = x * gamma * rsqrt(mean(x^2) + eps)                  */
/* ------------------------------------------------------------------ */

static void rmsnorm(
    const float *input, const float *gamma,
    float *output, int dim
) {
    /* sum of squares via FVMAC(input, input) */
    NPU_FVMAC(input, input, dim);
    float sum_sq = NPU_FRSTACC();
    float mean_sq = sum_sq / (float)dim + 1e-5f;

    /* 1/sqrt(mean_sq) */
    float scale = NPU_FVRSQRT(&mean_sq);

    /* output[i] = input[i] * gamma[i] * scale */
    for (int i = 0; i < dim; i++) {
        output[i] = input[i] * gamma[i] * scale;
    }
}

/* ------------------------------------------------------------------ */
/* Softmax: scores[] -> float probabilities                            */
/* ------------------------------------------------------------------ */

static void softmax(float *scores, int n, float *probs) {
    float max_score = NPU_FVMAX(scores, n);

    /* subtract max */
    for (int i = 0; i < n; i++)
        scores[i] -= max_score;

    /* exp */
    NPU_FVEXP(scores, probs, n);

    /* sum */
    float sum_exp = NPU_FVREDUCE(probs, n);

    /* normalize: set facc = 1/sum, then FVMUL */
    NPU_FRSTACC();                       /* clear facc */
    float inv = 1.0f / sum_exp;
    float one = 1.0f;
    NPU_FMACC(inv, one);                /* facc = 1/sum */
    NPU_FVMUL(probs, probs, n);         /* probs[i] *= facc */
}

/* ------------------------------------------------------------------ */
/* GELU activation                                                     */
/* ------------------------------------------------------------------ */

static void gelu_activation(float *buf, int n) {
    for (int i = 0; i < n; i++)
        buf[i] = NPU_FGELU(buf[i]);
}

/* ------------------------------------------------------------------ */
/* Multi-head attention                                                */
/* ------------------------------------------------------------------ */

static void attention(
    const float *x_in,
    int layer,
    int pos,
    const float *wq, const float *bq,
    const float *wk, const float *bk,
    const float *wv, const float *bv,
    const float *wo, const float *bo,
    float *output
) {
    /* Project to Q, K, V */
    linear(x_in, EMBED_DIM, wq, bq, q_proj, EMBED_DIM);
    linear(x_in, EMBED_DIM, wk, bk, k_proj, EMBED_DIM);
    linear(x_in, EMBED_DIM, wv, bv, v_proj, EMBED_DIM);

    /* Store K, V into cache at position pos */
    for (int d = 0; d < EMBED_DIM; d++) {
        k_cache[layer][pos][d] = k_proj[d];
        v_cache[layer][pos][d] = v_proj[d];
    }

    int n_tokens = pos + 1;
    float head_dim_f = (float)HEAD_DIM;
    float attn_scale = NPU_FVRSQRT(&head_dim_f);

    /* Per-head attention */
    for (int h = 0; h < N_HEADS; h++) {
        int h_start = h * HEAD_DIM;

        /* Compute attention scores: Q[h] . K_cache[h] * scale */
        for (int t = 0; t < n_tokens; t++) {
            NPU_FVMAC(&q_proj[h_start], &k_cache[layer][t][h_start], HEAD_DIM);
            scores_buf[t] = NPU_FRSTACC() * attn_scale;
        }

        /* Softmax */
        softmax(scores_buf, n_tokens, probs_buf);

        /* Weighted sum of V */
        for (int d = 0; d < HEAD_DIM; d++) {
            float acc = 0.0f;
            for (int t = 0; t < n_tokens; t++) {
                acc += probs_buf[t] * v_cache[layer][t][h_start + d];
            }
            attn_out[h_start + d] = acc;
        }
    }

    /* Output projection */
    linear(attn_out, EMBED_DIM, wo, bo, output, EMBED_DIM);
}

/* ------------------------------------------------------------------ */
/* Feedforward: GELU(x @ W1 + b1) @ W2 + b2                           */
/* ------------------------------------------------------------------ */

static void feedforward(
    const float *x_in,
    const float *w1, const float *b1,
    const float *w2, const float *b2,
    float *output
) {
    /* First linear: dim -> ff_dim */
    linear(x_in, EMBED_DIM, w1, b1, ff_hidden, FF_DIM);

    /* GELU activation */
    gelu_activation(ff_hidden, FF_DIM);

    /* Second linear: ff_dim -> dim */
    linear(ff_hidden, FF_DIM, w2, b2, output, EMBED_DIM);
}

/* ------------------------------------------------------------------ */
/* Add residual                                                        */
/* ------------------------------------------------------------------ */

static void add_residual(float *x_inout, const float *residual, int dim) {
    for (int i = 0; i < dim; i++) {
        x_inout[i] = x_inout[i] + residual[i];
    }
}

/* ------------------------------------------------------------------ */
/* Main: transformer forward pass                                      */
/* ------------------------------------------------------------------ */

int main(void) {
    int n_tokens = test_n_tokens;
    if (n_tokens <= 0 || n_tokens > CONTEXT_LEN) {
        _exit(1);
    }

    /* Process each token */
    for (int pos = 0; pos < n_tokens; pos++) {
        unsigned char token = test_tokens[pos];

        /* Embedding: token_embed[token] + pos_embed[pos] */
        for (int d = 0; d < EMBED_DIM; d++) {
            x[d] = TOKEN_EMBED[token][d] + POS_EMBED[pos][d];
        }

        /* Layer 0 */
        {
            float residual[EMBED_DIM];
            for (int d = 0; d < EMBED_DIM; d++) residual[d] = x[d];

            /* Attention sub-layer */
            rmsnorm(x, L0_LN1_GAMMA, normed, EMBED_DIM);
            float attn_result[EMBED_DIM];
            attention(normed, 0, pos,
                      (const float *)L0_WQ, L0_BQ,
                      (const float *)L0_WK, L0_BK,
                      (const float *)L0_WV, L0_BV,
                      (const float *)L0_WO, L0_BO,
                      attn_result);
            for (int d = 0; d < EMBED_DIM; d++) x[d] = attn_result[d];
            add_residual(x, residual, EMBED_DIM);

            /* FFN sub-layer */
            for (int d = 0; d < EMBED_DIM; d++) residual[d] = x[d];
            rmsnorm(x, L0_LN2_GAMMA, normed, EMBED_DIM);
            float ff_result[EMBED_DIM];
            feedforward(normed,
                        (const float *)L0_W1, L0_B1,
                        (const float *)L0_W2, L0_B2,
                        ff_result);
            for (int d = 0; d < EMBED_DIM; d++) x[d] = ff_result[d];
            add_residual(x, residual, EMBED_DIM);
        }

        /* Layer 1 */
        {
            float residual[EMBED_DIM];
            for (int d = 0; d < EMBED_DIM; d++) residual[d] = x[d];

            rmsnorm(x, L1_LN1_GAMMA, normed, EMBED_DIM);
            float attn_result[EMBED_DIM];
            attention(normed, 1, pos,
                      (const float *)L1_WQ, L1_BQ,
                      (const float *)L1_WK, L1_BK,
                      (const float *)L1_WV, L1_BV,
                      (const float *)L1_WO, L1_BO,
                      attn_result);
            for (int d = 0; d < EMBED_DIM; d++) x[d] = attn_result[d];
            add_residual(x, residual, EMBED_DIM);

            for (int d = 0; d < EMBED_DIM; d++) residual[d] = x[d];
            rmsnorm(x, L1_LN2_GAMMA, normed, EMBED_DIM);
            float ff_result[EMBED_DIM];
            feedforward(normed,
                        (const float *)L1_W1, L1_B1,
                        (const float *)L1_W2, L1_B2,
                        ff_result);
            for (int d = 0; d < EMBED_DIM; d++) x[d] = ff_result[d];
            add_residual(x, residual, EMBED_DIM);
        }
    }

    /* Final RMSNorm */
    rmsnorm(x, LN_FINAL_GAMMA, normed, EMBED_DIM);

    /* Output projection: find argmax of logits */
    float max_logit = -1e30f;
    int predicted = 0;
    for (int v = 0; v < VOCAB_SIZE; v++) {
        NPU_FVMAC(&OUTPUT_PROJ[v][0], normed, EMBED_DIM);
        float logit = NPU_FRSTACC() + OUTPUT_BIAS[v];
        if (logit > max_logit) {
            max_logit = logit;
            predicted = v;
        }
    }

    print_int(predicted);
    return 0;
}
