/* Tiny transformer character-level LM demo.
 *
 * Reads input tokens from the test_tokens buffer (written by the test
 * harness), runs one forward pass of the quantized transformer, and
 * prints the predicted next token (byte value 0-255) to stdout.
 *
 * Architecture: embed_dim=64, heads=4, layers=2, vocab=256, ctx=32
 * All weights are int8, intermediates are int32/Q16.16.
 */

#include <stdint.h>
#include "../common/npu.h"

/* Weights header (generated by riscv_npu.tools.export_transformer_weights) */
#include "weights.h"

/* Syscall wrappers */
long write(int fd, const void *buf, long len);
void _exit(int code);

/* Input buffer: test harness writes token IDs here.
 * test_n_tokens: number of tokens to process. */
uint8_t test_tokens[CONTEXT_LEN];
int32_t test_n_tokens;

/* Scratch buffers (in .bss) */
static int8_t x[EMBED_DIM];           /* Current token embedding */
static int8_t normed[EMBED_DIM];      /* After RMSNorm */
static int8_t q_proj[EMBED_DIM];      /* Q projection */
static int8_t k_proj[EMBED_DIM];      /* K projection */
static int8_t v_proj[EMBED_DIM];      /* V projection */
static int8_t attn_out[EMBED_DIM];    /* Attention output */
static int8_t ff_hidden[FF_DIM];      /* FFN hidden */
static int8_t ff_out[EMBED_DIM];      /* FFN output */

/* KV cache: (n_layers, context_len, embed_dim) */
static int8_t k_cache[N_LAYERS][CONTEXT_LEN][EMBED_DIM];
static int8_t v_cache[N_LAYERS][CONTEXT_LEN][EMBED_DIM];

/* Temporary int32 buffers for softmax */
static int32_t scores_buf[CONTEXT_LEN];
static int32_t exp_buf[CONTEXT_LEN];

/* ------------------------------------------------------------------ */
/* Utility                                                             */
/* ------------------------------------------------------------------ */

static inline int8_t clamp_i8(int32_t x) {
    return (int8_t)NPU_CLAMP(x);
}

static void print_int(int n) {
    char buf[8];
    int len = 0;
    if (n >= 100) {
        buf[len++] = '0' + (n / 100);
        n %= 100;
        buf[len++] = '0' + (n / 10);
        buf[len++] = '0' + (n % 10);
    } else if (n >= 10) {
        buf[len++] = '0' + (n / 10);
        buf[len++] = '0' + (n % 10);
    } else {
        buf[len++] = '0' + n;
    }
    buf[len++] = '\n';
    write(1, buf, len);
}

/* ------------------------------------------------------------------ */
/* Linear layer: out = clamp((W @ in + bias) >> shift)                 */
/* ------------------------------------------------------------------ */

static void linear(
    const int8_t *input, int in_dim,
    const int8_t *weight, const int32_t *bias,
    int8_t *output, int out_dim,
    int shift
) {
    for (int i = 0; i < out_dim; i++) {
        NPU_VMAC(&weight[i * in_dim], input, in_dim);
        int32_t acc = NPU_RSTACC();
        acc += bias[i];
        acc >>= shift;
        output[i] = clamp_i8(acc);
    }
}

/* ------------------------------------------------------------------ */
/* RMSNorm: out = x * gamma * rsqrt(mean(x^2) + eps)                  */
/* ------------------------------------------------------------------ */

static void rmsnorm(
    const int8_t *input,
    const int8_t *gamma,
    int8_t *output,
    int dim
) {
    /* Compute sum of squares into int32 buffer (reuse scores_buf) */
    int32_t sum_sq = 0;
    for (int i = 0; i < dim; i++) {
        sum_sq += (int32_t)input[i] * (int32_t)input[i];
    }

    /* mean_sq in Q16.16: (sum_sq * 65536) / dim */
    int32_t mean_sq_q = (sum_sq * 65536) / dim;
    /* Add epsilon */
    mean_sq_q += 1;

    /* Store mean_sq to memory for VRSQRT */
    int32_t rsqrt_input = mean_sq_q;
    int32_t scale = NPU_VRSQRT(&rsqrt_input);

    /* Apply: out[i] = clamp((input[i] * gamma[i] * scale) >> 16) */
    for (int i = 0; i < dim; i++) {
        int32_t xg = (int32_t)input[i] * (int32_t)gamma[i];
        int32_t val = (xg * scale) >> 16;
        output[i] = clamp_i8(val);
    }
}

/* ------------------------------------------------------------------ */
/* Softmax: scores[] -> uint8 probabilities                            */
/* ------------------------------------------------------------------ */

static void softmax(
    int32_t *scores, int n,
    uint8_t *probs
) {
    /* Find max using VMAX */
    int32_t max_score = NPU_VMAX(scores, n);

    /* Subtract max and convert to Q16.16 */
    for (int i = 0; i < n; i++) {
        scores[i] = (scores[i] - max_score) << 16;  /* Q16.16, always <= 0 */
    }

    /* Exp in Q16.16 */
    NPU_VEXP(scores, exp_buf, n);

    /* Sum using VREDUCE */
    int32_t sum_exp = NPU_VREDUCE(exp_buf, n);

    /* Normalize to uint8 [0, 255] */
    if (sum_exp == 0) sum_exp = 1;
    for (int i = 0; i < n; i++) {
        probs[i] = (uint8_t)((exp_buf[i] * 255 + sum_exp / 2) / sum_exp);
    }
}

/* ------------------------------------------------------------------ */
/* Multi-head attention                                                */
/* ------------------------------------------------------------------ */

static void attention(
    const int8_t *x_in,
    int layer,
    int pos,
    const int8_t *wq, const int32_t *bq,
    const int8_t *wk, const int32_t *bk,
    const int8_t *wv, const int32_t *bv,
    const int8_t *wo, const int32_t *bo,
    int32_t attn_scale_q16,
    int proj_shift,
    int8_t *output
) {
    /* Project to Q, K, V */
    linear(x_in, EMBED_DIM, wq, bq, q_proj, EMBED_DIM, proj_shift);
    linear(x_in, EMBED_DIM, wk, bk, k_proj, EMBED_DIM, proj_shift);
    linear(x_in, EMBED_DIM, wv, bv, v_proj, EMBED_DIM, proj_shift);

    /* Store K, V into cache at position pos */
    for (int d = 0; d < EMBED_DIM; d++) {
        k_cache[layer][pos][d] = k_proj[d];
        v_cache[layer][pos][d] = v_proj[d];
    }

    int n_tokens = pos + 1;
    uint8_t probs[CONTEXT_LEN];

    /* Per-head attention */
    for (int h = 0; h < N_HEADS; h++) {
        int h_start = h * HEAD_DIM;

        /* Compute attention scores: Q[h] @ K_cache[h]^T */
        for (int t = 0; t < n_tokens; t++) {
            /* Dot product of q_proj[h_start..h_start+HEAD_DIM] with
             * k_cache[layer][t][h_start..h_start+HEAD_DIM] */
            NPU_VMAC(&q_proj[h_start], &k_cache[layer][t][h_start], HEAD_DIM);
            int32_t score = NPU_RSTACC();
            /* Scale by 1/sqrt(head_dim) in Q16.16 */
            score = (score * attn_scale_q16) >> 16;
            scores_buf[t] = score;
        }

        /* Softmax */
        softmax(scores_buf, n_tokens, probs);

        /* Weighted sum of V */
        for (int d = 0; d < HEAD_DIM; d++) {
            int32_t acc = 0;
            for (int t = 0; t < n_tokens; t++) {
                acc += probs[t] * (int32_t)v_cache[layer][t][h_start + d];
            }
            /* Normalize: probs are uint8 [0,255] */
            attn_out[h_start + d] = clamp_i8((acc + 127) / 255);
        }
    }

    /* Output projection */
    linear(attn_out, EMBED_DIM, wo, bo, output, EMBED_DIM, proj_shift);
}

/* ------------------------------------------------------------------ */
/* Feedforward: GELU(x @ W1 + b1) @ W2 + b2                           */
/* ------------------------------------------------------------------ */

static void feedforward(
    const int8_t *x_in,
    const int8_t *w1, const int32_t *b1,
    const int8_t *w2, const int32_t *b2,
    int ff_shift,
    int8_t *output
) {
    /* First linear: dim -> ff_dim */
    linear(x_in, EMBED_DIM, w1, b1, ff_hidden, FF_DIM, ff_shift);

    /* GELU activation */
    for (int i = 0; i < FF_DIM; i++) {
        ff_hidden[i] = (int8_t)NPU_GELU(ff_hidden[i]);
    }

    /* Second linear: ff_dim -> dim */
    linear(ff_hidden, FF_DIM, w2, b2, output, EMBED_DIM, ff_shift);
}

/* ------------------------------------------------------------------ */
/* Add residual                                                        */
/* ------------------------------------------------------------------ */

static void add_residual(int8_t *x_inout, const int8_t *residual, int dim) {
    for (int i = 0; i < dim; i++) {
        x_inout[i] = clamp_i8((int32_t)x_inout[i] + (int32_t)residual[i]);
    }
}

/* ------------------------------------------------------------------ */
/* Main: transformer forward pass                                      */
/* ------------------------------------------------------------------ */

int main(void) {
    int n_tokens = test_n_tokens;
    if (n_tokens <= 0 || n_tokens > CONTEXT_LEN) {
        _exit(1);
    }

    /* Process each token */
    for (int pos = 0; pos < n_tokens; pos++) {
        uint8_t token = test_tokens[pos];

        /* Embedding: token_embed[token] + pos_embed[pos] */
        for (int d = 0; d < EMBED_DIM; d++) {
            x[d] = clamp_i8(
                (int32_t)TOKEN_EMBED[token][d] + (int32_t)POS_EMBED[pos][d]
            );
        }

        /* Layer 0 */
        {
            int8_t residual[EMBED_DIM];
            for (int d = 0; d < EMBED_DIM; d++) residual[d] = x[d];

            /* Attention sub-layer */
            rmsnorm(x, L0_LN1_GAMMA, normed, EMBED_DIM);
            int8_t attn_result[EMBED_DIM];
            attention(normed, 0, pos,
                      (const int8_t *)L0_WQ, L0_BQ,
                      (const int8_t *)L0_WK, L0_BK,
                      (const int8_t *)L0_WV, L0_BV,
                      (const int8_t *)L0_WO, L0_BO,
                      L0_ATTN_SCALE, L0_PROJ_SHIFT,
                      attn_result);
            for (int d = 0; d < EMBED_DIM; d++) x[d] = attn_result[d];
            add_residual(x, residual, EMBED_DIM);

            /* FFN sub-layer */
            for (int d = 0; d < EMBED_DIM; d++) residual[d] = x[d];
            rmsnorm(x, L0_LN2_GAMMA, normed, EMBED_DIM);
            int8_t ff_result[EMBED_DIM];
            feedforward(normed,
                        (const int8_t *)L0_W1, L0_B1,
                        (const int8_t *)L0_W2, L0_B2,
                        L0_FF_SHIFT, ff_result);
            for (int d = 0; d < EMBED_DIM; d++) x[d] = ff_result[d];
            add_residual(x, residual, EMBED_DIM);
        }

        /* Layer 1 */
        {
            int8_t residual[EMBED_DIM];
            for (int d = 0; d < EMBED_DIM; d++) residual[d] = x[d];

            rmsnorm(x, L1_LN1_GAMMA, normed, EMBED_DIM);
            int8_t attn_result[EMBED_DIM];
            attention(normed, 1, pos,
                      (const int8_t *)L1_WQ, L1_BQ,
                      (const int8_t *)L1_WK, L1_BK,
                      (const int8_t *)L1_WV, L1_BV,
                      (const int8_t *)L1_WO, L1_BO,
                      L1_ATTN_SCALE, L1_PROJ_SHIFT,
                      attn_result);
            for (int d = 0; d < EMBED_DIM; d++) x[d] = attn_result[d];
            add_residual(x, residual, EMBED_DIM);

            for (int d = 0; d < EMBED_DIM; d++) residual[d] = x[d];
            rmsnorm(x, L1_LN2_GAMMA, normed, EMBED_DIM);
            int8_t ff_result[EMBED_DIM];
            feedforward(normed,
                        (const int8_t *)L1_W1, L1_B1,
                        (const int8_t *)L1_W2, L1_B2,
                        L1_FF_SHIFT, ff_result);
            for (int d = 0; d < EMBED_DIM; d++) x[d] = ff_result[d];
            add_residual(x, residual, EMBED_DIM);
        }
    }

    /* Final RMSNorm */
    rmsnorm(x, LN_FINAL_GAMMA, normed, EMBED_DIM);

    /* Output projection: find argmax of logits */
    int32_t max_logit = -0x7FFFFFFF;
    int predicted = 0;
    for (int v = 0; v < VOCAB_SIZE; v++) {
        NPU_VMAC(&OUTPUT_PROJ[v][0], normed, EMBED_DIM);
        int32_t logit = NPU_RSTACC() + OUTPUT_BIAS[v];
        if (logit > max_logit) {
            max_logit = logit;
            predicted = v;
        }
    }

    print_int(predicted);
    return 0;
}
