# Phase 6: MNIST Inference

## Goal
Quantized MLP runs on emulator, correctly classifies digits.

## What to build
- tools/export_weights.py: train MLP (784->128 ReLU->10) on MNIST, quantize int8, export as C arrays
- firmware/mnist/weights.h: generated weight arrays
- firmware/mnist/nn_runtime.c: linear layer + activation using NPU_MACC, NPU_RSTACC, NPU_RELU, NPU_CLAMP
- firmware/mnist/main.c: load test image, run inference, print prediction
- tests/integration/test_mnist.py: run multiple images, compare to PyTorch reference

## Network
- Input: 784 int8 (28x28 flattened)
- Hidden: 128, ReLU
- Output: 10, argmax
- ~100KB weights

## Design Decisions

### Quantization scheme
- **Per-layer symmetric int8 quantization**: weights are quantized to int8 [-128, 127] using a per-layer scale factor.
- **Scale factor encoding**: stored as int32 fixed-point (scale * 256) so QMUL can rescale accumulator results.
- **Biases stored as int32**: biases are pre-scaled to match the accumulator scale (input_scale * weight_scale), avoiding runtime rescaling of bias.
- **Activations**: input pixels are uint8 [0, 255] -> int8 [-128, 127] by subtracting 128. Hidden activations are int8 after CLAMP.

### Firmware memory layout
- Weights and biases are compiled into the ELF as const arrays in weights.h (~102KB total).
- Test image (784 bytes) loaded at a known symbol `test_image` in .bss.
- The test harness writes image bytes into RAM at the `test_image` address before running.
- Output: firmware prints the predicted digit (0-9) as an ASCII character followed by newline.

### NPU usage in linear layer
- For each output neuron: reset accumulator (RSTACC), then MACC over input*weight pairs, then RSTACC to read result.
- Process 1 element at a time: MACC(input[j], weight[i][j]) for all j, then read accumulator.
- After accumulation: add bias (int32), apply QMUL with output_scale to rescale, CLAMP to int8.
- After layer 1: apply RELU. Layer 2 has no activation (raw scores for argmax).

### Test approach
- export_weights.py saves: weights.h (C header), test_data.py (Python module with quantized weights, test images, and PyTorch reference predictions).
- Integration test loads the pre-exported test_data.py, writes each image into ELF memory, runs inference, compares to reference.
- Accuracy target: >=95% on 100 test images.
- Test also verifies single known-good image for deterministic correctness.

## Deliverables List

| # | Deliverable                                   | Dependencies |
|---|-----------------------------------------------|--------------|
| 1 | tools/export_weights.py                       | none         |
| 2 | firmware/mnist/weights.h + test data          | D1           |
| 3 | firmware/mnist/nn_runtime.c + nn_runtime.h    | D2           |
| 4 | firmware/mnist/main.c + Makefile              | D3           |
| 5 | tests/integration/test_mnist.py               | D2, D4       |

## Implementation Details

### D1: tools/export_weights.py
**Files**: `tools/export_weights.py`

**Functions**:
- `train_model() -> torch.nn.Module`: Train 784->128->10 MLP on MNIST to ~97% accuracy. Uses Adam, 5 epochs, CrossEntropyLoss.
- `quantize_weights(model) -> dict`: Extract and quantize each layer's weights to int8 with per-layer scale. Returns dict with keys: w1 (int8 784x128), b1 (int32 128), s1 (int32 scale), w2 (int8 128x10), b2 (int32 10), s2 (int32 scale).
- `export_c_header(weights: dict, path: str) -> None`: Write weights.h with C arrays.
- `export_test_data(model, weights: dict, path: str) -> None`: Save Python module with quantized weights, 100 test images (int8), their labels, and PyTorch float-model predictions.
- `main()`: Orchestrate training, quantization, export.

**Quantization math**:
- `scale = 127.0 / max(abs(weight_tensor))`
- `quantized = clamp(round(weight * scale), -128, 127)`
- Output scale for QMUL: `round(1.0 / (input_scale * weight_scale) * 256)` -- this rescales the int32 accumulator back to int8 range.
- Bias quantization: `bias_q = round(bias * input_scale * weight_scale)` as int32.

### D2: firmware/mnist/weights.h
**Files**: `firmware/mnist/weights.h`

Generated by export_weights.py. Contains:
```c
#ifndef WEIGHTS_H
#define WEIGHTS_H
#include <stdint.h>

// Layer 1: 784 -> 128
static const int8_t W1[128][784] = { ... };
static const int32_t B1[128] = { ... };
static const int32_t S1 = ...; // output scale for QMUL

// Layer 2: 128 -> 10
static const int8_t W2[10][128] = { ... };
static const int32_t B2[10] = { ... };
static const int32_t S2 = ...; // output scale for QMUL
#endif
```

### D3: firmware/mnist/nn_runtime.c + nn_runtime.h
**Files**: `firmware/mnist/nn_runtime.c`, `firmware/mnist/nn_runtime.h`

**Functions**:
```c
// Apply a linear layer: out[out_dim] = W[out_dim][in_dim] @ in[in_dim] + bias[out_dim]
// Then rescale with QMUL(result, scale) and CLAMP to int8.
void linear(const int8_t *input, const int8_t *weights, const int32_t *bias,
            int32_t scale, int out_dim, int in_dim, int8_t *output);

// Apply ReLU activation in-place on int8 array.
void relu(int8_t *data, int len);

// Find index of maximum value in int8 array.
int argmax(const int8_t *data, int len);

// Run full inference: input[784] -> predicted digit [0-9].
int inference(const int8_t *image);
```

**linear() implementation** (using NPU):
```c
for (int i = 0; i < out_dim; i++) {
    NPU_RSTACC();  // clear accumulator
    for (int j = 0; j < in_dim; j++) {
        NPU_MACC((int32_t)input[j], (int32_t)weights[i * in_dim + j]);
    }
    int32_t acc = NPU_RSTACC();  // read accumulated dot product
    acc += bias[i];              // add bias (already in accumulator scale)
    acc = NPU_QMUL(acc, scale); // rescale to output int8 range
    output[i] = (int8_t)NPU_CLAMP(acc);  // clamp to [-128, 127]
}
```

### D4: firmware/mnist/main.c + Makefile
**Files**: `firmware/mnist/main.c`, `firmware/mnist/Makefile`

**main.c**:
```c
#include "nn_runtime.h"
#include "weights.h"
#include "../common/npu.h"

int8_t test_image[784];  // Written by test harness before run

int main(void) {
    int digit = inference(test_image);
    // Print digit as ASCII + newline
    char buf[3];
    if (digit >= 10) { buf[0] = '1'; buf[1] = '0' + (digit - 10); buf[2] = '\n'; write(1, buf, 3); }
    else { buf[0] = '0' + digit; buf[1] = '\n'; write(1, buf, 2); }
    return 0;
}
```

**Makefile**: follows pattern from firmware/npu_test/Makefile. Links start.o + main.o + nn_runtime.o + syscalls.o -> mnist.elf.

### D5: tests/integration/test_mnist.py
**Files**: `tests/integration/test_mnist.py`

**Test functions**:
- `test_mnist_single_image()`: Run one known image, verify correct digit output.
- `test_mnist_accuracy()`: Run 100 test images, verify >=95% match PyTorch reference.
- `test_mnist_all_digits_represented()`: Verify test set covers all 10 digits.

**Approach**:
- Load pre-exported test data (images + reference predictions) from `firmware/mnist/test_data.py`.
- For each image: find `test_image` symbol in ELF, write 784 int8 bytes to that address, run CPU, capture stdout, parse digit.
- Compare to PyTorch reference prediction.

## Test Coverage Requirements

### D1: export_weights.py
- No pytest tests (interactive tool). Verified by running it and checking outputs exist.

### D3-D5: Integration tests
- `test_mnist_single_image`: loads one image, runs ELF, checks printed digit matches reference.
- `test_mnist_accuracy`: runs 100 images, asserts >=95% correct vs PyTorch reference.
- `test_mnist_all_digits_represented`: test image set has at least one of each digit 0-9.

## Acceptance Criteria

1. `uv run python tools/export_weights.py` generates `firmware/mnist/weights.h` and `firmware/mnist/test_data.py`
2. `cd firmware/mnist && make` produces `mnist.elf` without errors
3. `uv run python -m riscv_npu run firmware/mnist/mnist.elf` prints a digit and exits cleanly
4. `uv run pytest tests/integration/test_mnist.py -v` passes with >=95% accuracy
5. `uv run pytest` shows all tests passing (555 + new tests)
